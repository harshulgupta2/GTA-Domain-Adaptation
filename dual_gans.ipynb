{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "data_root = \"/datasets/home/32/232/tdobhal/Project/6_train/images/\"\n",
    "\n",
    "# Number of images in the directory\n",
    "num_images = 23418\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 16\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this size using a transformer.\n",
    "image_size = 256\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.00005\n",
    "\n",
    "# Alpha hyperparam for RMS optimizers\n",
    "alpha = 0.9\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = torch.cuda.device_count()\n",
    "\n",
    "# Device to run on\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Parameters:\n",
    "        \n",
    "        '''\n",
    "        self.device = device\n",
    "        self.data_path = data_root\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.train_names = glob.glob(self.data_path + 'real_A/*')\n",
    "        self.names = [self.train_names[i].split('/')[-1] for i in range(len(self.train_names))]\n",
    "        self.data_transforms = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.Resize(image_size),\n",
    "                torchvision.transforms.CenterCrop(image_size),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "    \n",
    "    def image_loader(self, image_name):\n",
    "        \"\"\"load image, returns cuda tensor\"\"\"\n",
    "        image = Image.open(image_name)\n",
    "        image = self.data_transforms(image).float()\n",
    "        image = torch.autograd.Variable(image, requires_grad=True)\n",
    "        image = image.unsqueeze(0)  # this is for VGG, may not be needed for ResNet\n",
    "        return image[0].to(self.device)  # assumes that you're using GPU\n",
    "\n",
    "    def show(self, img):\n",
    "        npimg = img.cpu().detach().numpy()\n",
    "        npimg = np.transpose(npimg, (1,2,0))\n",
    "        if npimg.shape[2] == 3:\n",
    "            plt.imshow(npimg)\n",
    "        else:\n",
    "            plt.imshow(npimg[:,:,0], cmap='gray')\n",
    "            \n",
    "    def imshow(self, img):\n",
    "        img = img / 2 + 0.5     # unnormalize\n",
    "        npimg = img.cpu().detach().numpy()\n",
    "        plt.figure(figsize = (10,2))\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)), aspect='auto')\n",
    "\n",
    "    def data_generator(self):\n",
    "        root = self.data_path\n",
    "        batch_size = self.batch_size\n",
    "        \n",
    "        images_dir = root + 'real_A/'\n",
    "        labels_dir = root + 'fake_B/'\n",
    "\n",
    "        while True:\n",
    "            x, y = [], []\n",
    "            idx = np.random.choice(self.names, batch_size)\n",
    "            for i in range(idx.shape[0]):\n",
    "                x.append(self.image_loader(images_dir + idx[i]))\n",
    "                y.append(self.image_loader(labels_dir + idx[i]))\n",
    "            yield torch.stack(x), torch.stack(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.01)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.01)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        # Convolution layers\n",
    "        \n",
    "        # input is (nc) x 256 x 256\n",
    "        self.conv1 = nn.Conv2d(nc, ngf, 4, 2, 1, bias=True)\n",
    "        self.lr1 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf) x 128 x 128\n",
    "        self.conv2 = nn.Conv2d(ngf, ngf*2, 4, 2, 1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(ngf*2)\n",
    "        self.lr2 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf*2) x 64 x 64\n",
    "        self.conv3 = nn.Conv2d(ngf*2, ngf*4, 4, 2, 1, bias=True)\n",
    "        self.bn3 = nn.BatchNorm2d(ngf*4)\n",
    "        self.lr3 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf*4) x 32 x 32\n",
    "        self.conv4 = nn.Conv2d(ngf*4, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.bn4 = nn.BatchNorm2d(ngf*8)\n",
    "        self.lr4 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf*8) x 16 x 16\n",
    "        self.conv5 = nn.Conv2d(ngf*8, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.bn5 = nn.BatchNorm2d(ngf*8)\n",
    "        self.lr5 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf*8) x 8 x 8\n",
    "        self.conv6 = nn.Conv2d(ngf*8, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.bn6 = nn.BatchNorm2d(ngf*8)\n",
    "        self.lr6 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf*8) x 4 x 4\n",
    "        self.conv7 = nn.Conv2d(ngf*8, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.bn7 = nn.BatchNorm2d(ngf*8)\n",
    "        self.lr7 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf*8) x 2 x 2\n",
    "        self.conv8 = nn.Conv2d(ngf*8, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.bn8 = nn.BatchNorm2d(ngf*8)\n",
    "        self.r8 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Transpose Convolutional Layers\n",
    "        \n",
    "        # input is (ngf*8) x 1 x 1\n",
    "        self.tr_conv1 = nn.ConvTranspose2d(ngf*8, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.tr_bn1 = nn.BatchNorm2d(ngf*8)\n",
    "        self.tr_r1 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf*8)*2 x 2 x 2\n",
    "        self.tr_conv2 = nn.ConvTranspose2d((ngf*8)*2, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.tr_bn2 = nn.BatchNorm2d(ngf*8)\n",
    "        self.tr_r2 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf*8)*2 x 4 x 4\n",
    "        self.tr_conv3 = nn.ConvTranspose2d((ngf*8)*2, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.tr_bn3 = nn.BatchNorm2d(ngf*8)\n",
    "        self.tr_r3 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf*8)*2 x 8 x 8\n",
    "        self.tr_conv4 = nn.ConvTranspose2d((ngf*8)*2, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.tr_bn4 = nn.BatchNorm2d(ngf*8)\n",
    "        self.tr_r4 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf*8)*2 x 16 x 16\n",
    "        self.tr_conv5 = nn.ConvTranspose2d((ngf*8)*2, ngf*4, 4, 2, 1, bias=True)\n",
    "        self.tr_bn5 = nn.BatchNorm2d(ngf*4)\n",
    "        self.tr_r5 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf*4)*2 x 32 x 32\n",
    "        self.tr_conv6 = nn.ConvTranspose2d((ngf*4)*2, ngf*2, 4, 2, 1, bias=True)\n",
    "        self.tr_bn6 = nn.BatchNorm2d(ngf*2)\n",
    "        self.tr_r6 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf*2)*2 x 64 x 64\n",
    "        self.tr_conv7 = nn.ConvTranspose2d((ngf*2)*2, ngf, 4, 2, 1, bias=True)\n",
    "        self.tr_bn7 = nn.BatchNorm2d(ngf)\n",
    "        self.tr_r7 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf)*2 x 128 x 128\n",
    "        self.tr_conv8 = nn.ConvTranspose2d((ngf)*2, nc, 4, 2, 1, bias=True)\n",
    "        self.out = nn.Tanh()\n",
    "        # state size. (nc) x 256 x 256\n",
    "    \n",
    "    def forward(self, x):\n",
    "        c1 = self.conv1(x)\n",
    "        c2 = self.bn2(self.conv2(self.lr1(c1)))\n",
    "        c3 = self.bn3(self.conv3(self.lr2(c2)))\n",
    "        c4 = self.bn4(self.conv4(self.lr3(c3)))\n",
    "        c5 = self.bn5(self.conv5(self.lr4(c4)))\n",
    "        c6 = self.bn6(self.conv6(self.lr5(c5)))\n",
    "        c7 = self.bn7(self.conv7(self.lr6(c6)))\n",
    "        c8 = self.bn8(self.conv8(self.lr7(c7)))\n",
    "        \n",
    "        t1 = self.tr_bn1(self.tr_conv1(self.r8(c8)))\n",
    "        t1 = torch.cat((t1, c7), dim=1)\n",
    "        t2 = self.tr_bn2(self.tr_conv2(self.tr_r1(t1)))\n",
    "        t2 = torch.cat((t2, c6), dim=1)\n",
    "        t3 = self.tr_bn3(self.tr_conv3(self.tr_r2(t2)))\n",
    "        t3 = torch.cat((t3, c5), dim=1)\n",
    "        t4 = self.tr_bn4(self.tr_conv4(self.tr_r3(t3)))\n",
    "        t4 = torch.cat((t4, c4), dim=1)\n",
    "        t5 = self.tr_bn5(self.tr_conv5(self.tr_r4(t4)))\n",
    "        t5 = torch.cat((t5, c3), dim=1)\n",
    "        t6 = self.tr_bn6(self.tr_conv6(self.tr_r5(t5)))\n",
    "        t6 = torch.cat((t6, c2), dim=1)\n",
    "        t7 = self.tr_bn7(self.tr_conv7(self.tr_r6(t6)))\n",
    "        t7 = torch.cat((t7, c1), dim=1)\n",
    "        t8 = self.tr_conv8(self.tr_r7(t7))\n",
    "        t8 = self.out(t8)\n",
    "        return t8\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_a = Generator().to(device)\n",
    "gen_b = Generator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # input is (nc) x 256 x 256\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 128 x 128\n",
    "            nn.MaxPool2d((2, 2)), \n",
    "            \n",
    "            # state size. (ndf) x 64 x 64\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=True),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. 32 x 32\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "    \n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=True),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=True),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, ndf * 8, 4, 2, 1, bias=True),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        \n",
    "        self.flat = nn.Linear(ndf * 8 * 2 * 2, 1)\n",
    "        self.out = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.flat(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_a = Discriminator().to(device)\n",
    "dis_b = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Discriminator(\n",
       "    (net): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "      (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): LeakyReLU(negative_slope=0.2, inplace)\n",
       "      (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (7): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): LeakyReLU(negative_slope=0.2, inplace)\n",
       "      (10): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): LeakyReLU(negative_slope=0.2, inplace)\n",
       "      (13): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (15): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    )\n",
       "    (flat): Linear(in_features=2048, out_features=1, bias=True)\n",
       "    (out): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_a = nn.DataParallel(gen_a, list(range(ngpu)))\n",
    "dis_a = nn.DataParallel(dis_a, list(range(ngpu)))\n",
    "gen_b = nn.DataParallel(gen_b, list(range(ngpu)))\n",
    "dis_b = nn.DataParallel(dis_b, list(range(ngpu)))\n",
    "\n",
    "gen_a.apply(weights_init_normal)\n",
    "dis_a.apply(weights_init_normal)\n",
    "gen_b.apply(weights_init_normal)\n",
    "dis_b.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "optim_gen_a = torch.optim.RMSprop(gen_a.parameters(), lr, alpha)\n",
    "optim_gen_b = torch.optim.RMSprop(gen_b.parameters(), lr, alpha)\n",
    "optim_dis_a = torch.optim.RMSprop(dis_a.parameters(), lr, alpha)\n",
    "optim_dis_b = torch.optim.RMSprop(dis_b.parameters(), lr, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 0/1463] [D_A loss: 1.346673] [D_B loss: 1.391420] [G_A loss: 5.447925, G_B loss: 5.793624]\n",
      "[Epoch 0/100] [Batch 1/1463] [D_A loss: 0.242034] [D_B loss: 0.355362] [G_A loss: 0.935152, G_B loss: 1.980854]\n",
      "[Epoch 0/100] [Batch 2/1463] [D_A loss: 1.637061] [D_B loss: 2.425555] [G_A loss: 5.972492, G_B loss: 6.427376]\n",
      "[Epoch 0/100] [Batch 3/1463] [D_A loss: 0.863290] [D_B loss: 1.159959] [G_A loss: 2.181315, G_B loss: 3.224128]\n",
      "[Epoch 0/100] [Batch 4/1463] [D_A loss: 0.321887] [D_B loss: 0.416616] [G_A loss: 3.973685, G_B loss: 4.434943]\n",
      "[Epoch 0/100] [Batch 5/1463] [D_A loss: 0.159926] [D_B loss: 0.370327] [G_A loss: 2.795234, G_B loss: 3.434728]\n",
      "[Epoch 0/100] [Batch 6/1463] [D_A loss: 0.306757] [D_B loss: 0.726021] [G_A loss: 5.074713, G_B loss: 4.986984]\n",
      "[Epoch 0/100] [Batch 7/1463] [D_A loss: 0.315539] [D_B loss: 0.826276] [G_A loss: 0.881441, G_B loss: 2.081090]\n",
      "[Epoch 0/100] [Batch 8/1463] [D_A loss: 0.491640] [D_B loss: 1.199163] [G_A loss: 4.870562, G_B loss: 5.278346]\n",
      "[Epoch 0/100] [Batch 9/1463] [D_A loss: 0.466692] [D_B loss: 0.929466] [G_A loss: 1.121387, G_B loss: 1.760730]\n",
      "[Epoch 0/100] [Batch 10/1463] [D_A loss: 0.692051] [D_B loss: 0.780960] [G_A loss: 4.334346, G_B loss: 6.089253]\n",
      "[Epoch 0/100] [Batch 11/1463] [D_A loss: 0.774853] [D_B loss: 0.468004] [G_A loss: 1.786100, G_B loss: 1.774645]\n",
      "[Epoch 0/100] [Batch 12/1463] [D_A loss: 0.797588] [D_B loss: 0.619598] [G_A loss: 4.166860, G_B loss: 5.719338]\n",
      "[Epoch 0/100] [Batch 13/1463] [D_A loss: 0.450081] [D_B loss: 0.586853] [G_A loss: 1.008301, G_B loss: 2.626907]\n",
      "[Epoch 0/100] [Batch 14/1463] [D_A loss: 0.404026] [D_B loss: 1.259429] [G_A loss: 5.407010, G_B loss: 5.029110]\n",
      "[Epoch 0/100] [Batch 15/1463] [D_A loss: 0.319167] [D_B loss: 1.485957] [G_A loss: 0.799995, G_B loss: 2.255701]\n",
      "[Epoch 0/100] [Batch 16/1463] [D_A loss: 0.407463] [D_B loss: 0.763888] [G_A loss: 3.612789, G_B loss: 4.916166]\n",
      "[Epoch 0/100] [Batch 17/1463] [D_A loss: 0.419557] [D_B loss: 0.611451] [G_A loss: 1.706409, G_B loss: 1.093402]\n",
      "[Epoch 0/100] [Batch 18/1463] [D_A loss: 0.917786] [D_B loss: 0.828059] [G_A loss: 4.115609, G_B loss: 6.033729]\n",
      "[Epoch 0/100] [Batch 19/1463] [D_A loss: 1.054720] [D_B loss: 0.675057] [G_A loss: 0.835917, G_B loss: 0.717161]\n",
      "[Epoch 0/100] [Batch 20/1463] [D_A loss: 1.143469] [D_B loss: 1.166093] [G_A loss: 5.052456, G_B loss: 5.648494]\n",
      "[Epoch 0/100] [Batch 21/1463] [D_A loss: 0.386453] [D_B loss: 1.096289] [G_A loss: 0.799100, G_B loss: 2.913540]\n",
      "[Epoch 0/100] [Batch 22/1463] [D_A loss: 0.263808] [D_B loss: 0.797526] [G_A loss: 3.746869, G_B loss: 2.563672]\n",
      "[Epoch 0/100] [Batch 23/1463] [D_A loss: 0.378820] [D_B loss: 0.615781] [G_A loss: 1.072466, G_B loss: 3.695063]\n",
      "[Epoch 0/100] [Batch 24/1463] [D_A loss: 0.334690] [D_B loss: 1.059942] [G_A loss: 4.413246, G_B loss: 2.734823]\n",
      "[Epoch 0/100] [Batch 25/1463] [D_A loss: 0.378459] [D_B loss: 0.771975] [G_A loss: 1.039206, G_B loss: 5.351313]\n",
      "[Epoch 0/100] [Batch 26/1463] [D_A loss: 1.785957] [D_B loss: 0.777543] [G_A loss: 3.632315, G_B loss: 0.105559]\n",
      "[Epoch 0/100] [Batch 27/1463] [D_A loss: 3.176981] [D_B loss: 0.388061] [G_A loss: 1.836748, G_B loss: 4.446268]\n",
      "[Epoch 0/100] [Batch 28/1463] [D_A loss: 0.466331] [D_B loss: 0.481873] [G_A loss: 3.238148, G_B loss: 2.210472]\n",
      "[Epoch 0/100] [Batch 29/1463] [D_A loss: 0.440097] [D_B loss: 0.421316] [G_A loss: 2.083588, G_B loss: 3.886088]\n",
      "[Epoch 0/100] [Batch 30/1463] [D_A loss: 0.628478] [D_B loss: 0.525684] [G_A loss: 1.774613, G_B loss: 1.296781]\n",
      "[Epoch 0/100] [Batch 31/1463] [D_A loss: 0.911503] [D_B loss: 0.731324] [G_A loss: 4.688385, G_B loss: 4.644718]\n",
      "[Epoch 0/100] [Batch 32/1463] [D_A loss: 0.546939] [D_B loss: 1.178662] [G_A loss: 0.181598, G_B loss: 1.338638]\n",
      "[Epoch 0/100] [Batch 33/1463] [D_A loss: 0.552045] [D_B loss: 2.339672] [G_A loss: 4.758432, G_B loss: 4.596324]\n",
      "[Epoch 0/100] [Batch 34/1463] [D_A loss: 0.741317] [D_B loss: 1.421478] [G_A loss: 0.560928, G_B loss: 0.452299]\n",
      "[Epoch 0/100] [Batch 35/1463] [D_A loss: 1.702697] [D_B loss: 1.209983] [G_A loss: 3.976275, G_B loss: 5.665280]\n",
      "[Epoch 0/100] [Batch 36/1463] [D_A loss: 1.673172] [D_B loss: 0.544356] [G_A loss: 1.396917, G_B loss: 0.749540]\n",
      "[Epoch 0/100] [Batch 37/1463] [D_A loss: 0.887130] [D_B loss: 0.518205] [G_A loss: 2.365451, G_B loss: 4.013085]\n",
      "[Epoch 0/100] [Batch 38/1463] [D_A loss: 0.392922] [D_B loss: 0.409311] [G_A loss: 2.745323, G_B loss: 2.083722]\n",
      "[Epoch 0/100] [Batch 39/1463] [D_A loss: 0.332866] [D_B loss: 0.537842] [G_A loss: 1.018800, G_B loss: 2.779499]\n",
      "[Epoch 0/100] [Batch 40/1463] [D_A loss: 0.299674] [D_B loss: 1.211906] [G_A loss: 5.528790, G_B loss: 3.091704]\n",
      "[Epoch 0/100] [Batch 41/1463] [D_A loss: 0.288929] [D_B loss: 1.670857] [G_A loss: 0.605940, G_B loss: 2.493633]\n",
      "[Epoch 0/100] [Batch 42/1463] [D_A loss: 0.473478] [D_B loss: 1.206707] [G_A loss: 3.905057, G_B loss: 3.125028]\n",
      "[Epoch 0/100] [Batch 43/1463] [D_A loss: 0.326143] [D_B loss: 0.693107] [G_A loss: 0.865213, G_B loss: 2.630728]\n",
      "[Epoch 0/100] [Batch 44/1463] [D_A loss: 0.408162] [D_B loss: 0.805732] [G_A loss: 3.662720, G_B loss: 2.952123]\n",
      "[Epoch 0/100] [Batch 45/1463] [D_A loss: 0.360150] [D_B loss: 0.673171] [G_A loss: 0.700067, G_B loss: 2.009673]\n",
      "[Epoch 0/100] [Batch 46/1463] [D_A loss: 0.573176] [D_B loss: 1.145768] [G_A loss: 4.851038, G_B loss: 4.243268]\n",
      "[Epoch 0/100] [Batch 47/1463] [D_A loss: 0.518970] [D_B loss: 2.014228] [G_A loss: 0.466553, G_B loss: 0.180032]\n",
      "[Epoch 0/100] [Batch 48/1463] [D_A loss: 2.283087] [D_B loss: 1.101858] [G_A loss: 3.175872, G_B loss: 7.469545]\n",
      "[Epoch 0/100] [Batch 49/1463] [D_A loss: 4.425859] [D_B loss: 0.469768] [G_A loss: 1.589022, G_B loss: 2.012727]\n",
      "[Epoch 0/100] [Batch 50/1463] [D_A loss: 0.585077] [D_B loss: 0.794485] [G_A loss: 1.379272, G_B loss: 2.800975]\n",
      "[Epoch 0/100] [Batch 51/1463] [D_A loss: 0.521918] [D_B loss: 0.680078] [G_A loss: 2.756462, G_B loss: 1.264346]\n",
      "[Epoch 0/100] [Batch 52/1463] [D_A loss: 0.784163] [D_B loss: 0.741066] [G_A loss: 0.882776, G_B loss: 4.348069]\n",
      "[Epoch 0/100] [Batch 53/1463] [D_A loss: 1.763248] [D_B loss: 0.719968] [G_A loss: 3.722136, G_B loss: 0.094870]\n",
      "[Epoch 0/100] [Batch 54/1463] [D_A loss: 2.224614] [D_B loss: 0.992337] [G_A loss: 0.392503, G_B loss: 3.890598]\n",
      "[Epoch 0/100] [Batch 55/1463] [D_A loss: 0.771240] [D_B loss: 1.570743] [G_A loss: 4.240308, G_B loss: 0.847227]\n",
      "[Epoch 0/100] [Batch 56/1463] [D_A loss: 0.891308] [D_B loss: 1.389174] [G_A loss: 0.404383, G_B loss: 3.918324]\n",
      "[Epoch 0/100] [Batch 57/1463] [D_A loss: 0.949843] [D_B loss: 1.677651] [G_A loss: 3.494938, G_B loss: 0.632069]\n",
      "[Epoch 0/100] [Batch 58/1463] [D_A loss: 1.253496] [D_B loss: 1.144029] [G_A loss: 0.354270, G_B loss: 4.539867]\n",
      "[Epoch 0/100] [Batch 59/1463] [D_A loss: 1.437099] [D_B loss: 1.240682] [G_A loss: 2.871534, G_B loss: 0.510168]\n",
      "[Epoch 0/100] [Batch 60/1463] [D_A loss: 1.068894] [D_B loss: 0.986160] [G_A loss: 0.643402, G_B loss: 3.808668]\n",
      "[Epoch 0/100] [Batch 61/1463] [D_A loss: 1.331372] [D_B loss: 0.937839] [G_A loss: 2.667937, G_B loss: 0.479821]\n",
      "[Epoch 0/100] [Batch 62/1463] [D_A loss: 1.237095] [D_B loss: 1.039479] [G_A loss: 0.462297, G_B loss: 3.131459]\n",
      "[Epoch 0/100] [Batch 63/1463] [D_A loss: 0.531011] [D_B loss: 1.706931] [G_A loss: 3.706609, G_B loss: 1.362901]\n",
      "[Epoch 0/100] [Batch 64/1463] [D_A loss: 0.564580] [D_B loss: 1.707656] [G_A loss: 0.349491, G_B loss: 3.224195]\n",
      "[Epoch 0/100] [Batch 65/1463] [D_A loss: 0.597211] [D_B loss: 1.424418] [G_A loss: 2.794853, G_B loss: 0.965366]\n",
      "[Epoch 0/100] [Batch 66/1463] [D_A loss: 0.958305] [D_B loss: 1.418468] [G_A loss: 0.402948, G_B loss: 4.036160]\n",
      "[Epoch 0/100] [Batch 67/1463] [D_A loss: 1.869424] [D_B loss: 1.390992] [G_A loss: 2.171318, G_B loss: 0.145154]\n",
      "[Epoch 0/100] [Batch 68/1463] [D_A loss: 1.983747] [D_B loss: 0.801111] [G_A loss: 1.234635, G_B loss: 3.347908]\n",
      "[Epoch 0/100] [Batch 69/1463] [D_A loss: 0.664744] [D_B loss: 0.889956] [G_A loss: 1.642052, G_B loss: 0.896414]\n",
      "[Epoch 0/100] [Batch 70/1463] [D_A loss: 0.736043] [D_B loss: 0.906908] [G_A loss: 1.524940, G_B loss: 2.918165]\n",
      "[Epoch 0/100] [Batch 71/1463] [D_A loss: 0.696628] [D_B loss: 0.941604] [G_A loss: 2.706590, G_B loss: 0.811601]\n",
      "[Epoch 0/100] [Batch 72/1463] [D_A loss: 0.834969] [D_B loss: 1.254091] [G_A loss: 0.311802, G_B loss: 3.476964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 73/1463] [D_A loss: 0.976657] [D_B loss: 1.838124] [G_A loss: 3.362282, G_B loss: 0.525094]\n",
      "[Epoch 0/100] [Batch 74/1463] [D_A loss: 1.616441] [D_B loss: 1.533950] [G_A loss: 0.264423, G_B loss: 4.584720]\n",
      "[Epoch 0/100] [Batch 75/1463] [D_A loss: 2.186547] [D_B loss: 1.633555] [G_A loss: 2.590693, G_B loss: 0.398574]\n",
      "[Epoch 0/100] [Batch 76/1463] [D_A loss: 1.072684] [D_B loss: 1.277294] [G_A loss: 0.543051, G_B loss: 2.677326]\n",
      "[Epoch 0/100] [Batch 77/1463] [D_A loss: 0.821480] [D_B loss: 1.206126] [G_A loss: 2.388671, G_B loss: 0.920285]\n",
      "[Epoch 0/100] [Batch 78/1463] [D_A loss: 0.731072] [D_B loss: 1.010149] [G_A loss: 0.661382, G_B loss: 3.255643]\n",
      "[Epoch 0/100] [Batch 79/1463] [D_A loss: 1.444063] [D_B loss: 1.456120] [G_A loss: 2.649773, G_B loss: 0.189719]\n",
      "[Epoch 0/100] [Batch 80/1463] [D_A loss: 1.968631] [D_B loss: 1.135415] [G_A loss: 0.599193, G_B loss: 3.246586]\n",
      "[Epoch 0/100] [Batch 81/1463] [D_A loss: 1.224807] [D_B loss: 1.165106] [G_A loss: 2.868992, G_B loss: 0.553371]\n",
      "[Epoch 0/100] [Batch 82/1463] [D_A loss: 1.114506] [D_B loss: 1.514359] [G_A loss: 0.318175, G_B loss: 1.963317]\n",
      "[Epoch 0/100] [Batch 83/1463] [D_A loss: 0.857670] [D_B loss: 1.724873] [G_A loss: 2.418146, G_B loss: 1.040009]\n",
      "[Epoch 0/100] [Batch 84/1463] [D_A loss: 0.899659] [D_B loss: 1.175534] [G_A loss: 0.619110, G_B loss: 2.554118]\n",
      "[Epoch 0/100] [Batch 85/1463] [D_A loss: 0.957014] [D_B loss: 1.216500] [G_A loss: 2.307571, G_B loss: 0.367212]\n",
      "[Epoch 0/100] [Batch 86/1463] [D_A loss: 1.488509] [D_B loss: 1.382051] [G_A loss: 0.322838, G_B loss: 3.270457]\n",
      "[Epoch 0/100] [Batch 87/1463] [D_A loss: 1.483181] [D_B loss: 1.589708] [G_A loss: 2.335769, G_B loss: 0.187822]\n",
      "[Epoch 0/100] [Batch 88/1463] [D_A loss: 1.851917] [D_B loss: 1.342376] [G_A loss: 0.512313, G_B loss: 2.784230]\n",
      "[Epoch 0/100] [Batch 89/1463] [D_A loss: 1.473348] [D_B loss: 1.648026] [G_A loss: 1.795933, G_B loss: 0.301839]\n",
      "[Epoch 0/100] [Batch 90/1463] [D_A loss: 1.348752] [D_B loss: 0.842165] [G_A loss: 1.357505, G_B loss: 2.420182]\n",
      "[Epoch 0/100] [Batch 91/1463] [D_A loss: 1.174318] [D_B loss: 1.261908] [G_A loss: 0.704614, G_B loss: 0.471275]\n",
      "[Epoch 0/100] [Batch 92/1463] [D_A loss: 1.393035] [D_B loss: 1.468135] [G_A loss: 1.794683, G_B loss: 2.537616]\n",
      "[Epoch 0/100] [Batch 93/1463] [D_A loss: 1.493018] [D_B loss: 1.334738] [G_A loss: 0.560216, G_B loss: 0.360642]\n",
      "[Epoch 0/100] [Batch 94/1463] [D_A loss: 1.444420] [D_B loss: 1.499475] [G_A loss: 2.043753, G_B loss: 2.239626]\n",
      "[Epoch 0/100] [Batch 95/1463] [D_A loss: 0.953003] [D_B loss: 1.336590] [G_A loss: 0.402758, G_B loss: 0.700584]\n",
      "[Epoch 0/100] [Batch 96/1463] [D_A loss: 1.054769] [D_B loss: 2.297030] [G_A loss: 2.168666, G_B loss: 1.619917]\n",
      "[Epoch 0/100] [Batch 97/1463] [D_A loss: 1.137319] [D_B loss: 1.339419] [G_A loss: 0.490286, G_B loss: 0.450065]\n",
      "[Epoch 0/100] [Batch 98/1463] [D_A loss: 1.562047] [D_B loss: 1.385716] [G_A loss: 1.848534, G_B loss: 3.089691]\n",
      "[Epoch 0/100] [Batch 99/1463] [D_A loss: 1.624585] [D_B loss: 1.677858] [G_A loss: 0.441173, G_B loss: 0.310108]\n",
      "[Epoch 0/100] [Batch 100/1463] [D_A loss: 1.356478] [D_B loss: 1.584832] [G_A loss: 1.900618, G_B loss: 1.972183]\n",
      "[Epoch 0/100] [Batch 101/1463] [D_A loss: 1.749866] [D_B loss: 1.820583] [G_A loss: 0.426182, G_B loss: 0.280832]\n",
      "[Epoch 0/100] [Batch 102/1463] [D_A loss: 1.812758] [D_B loss: 1.847326] [G_A loss: 1.480238, G_B loss: 2.029148]\n",
      "[Epoch 0/100] [Batch 103/1463] [D_A loss: 1.568097] [D_B loss: 1.564064] [G_A loss: 0.801578, G_B loss: 0.461698]\n",
      "[Epoch 0/100] [Batch 104/1463] [D_A loss: 1.464902] [D_B loss: 1.432904] [G_A loss: 1.234523, G_B loss: 1.690108]\n",
      "[Epoch 0/100] [Batch 105/1463] [D_A loss: 1.199165] [D_B loss: 1.237948] [G_A loss: 0.915753, G_B loss: 0.716086]\n",
      "[Epoch 0/100] [Batch 106/1463] [D_A loss: 1.255372] [D_B loss: 1.658651] [G_A loss: 0.967313, G_B loss: 1.156617]\n",
      "[Epoch 0/100] [Batch 107/1463] [D_A loss: 1.045525] [D_B loss: 1.390683] [G_A loss: 1.032993, G_B loss: 1.298778]\n",
      "[Epoch 0/100] [Batch 108/1463] [D_A loss: 0.976157] [D_B loss: 1.334793] [G_A loss: 1.138216, G_B loss: 1.310159]\n",
      "[Epoch 0/100] [Batch 109/1463] [D_A loss: 1.519078] [D_B loss: 1.394312] [G_A loss: 0.743358, G_B loss: 0.284260]\n",
      "[Epoch 0/100] [Batch 110/1463] [D_A loss: 1.934795] [D_B loss: 1.512724] [G_A loss: 1.208725, G_B loss: 2.250783]\n",
      "[Epoch 0/100] [Batch 111/1463] [D_A loss: 1.704053] [D_B loss: 1.568440] [G_A loss: 1.151497, G_B loss: 0.281186]\n",
      "[Epoch 0/100] [Batch 112/1463] [D_A loss: 1.685825] [D_B loss: 1.395854] [G_A loss: 0.913149, G_B loss: 1.905245]\n",
      "[Epoch 0/100] [Batch 113/1463] [D_A loss: 1.365196] [D_B loss: 1.323244] [G_A loss: 1.224848, G_B loss: 0.553560]\n",
      "[Epoch 0/100] [Batch 114/1463] [D_A loss: 1.372937] [D_B loss: 1.436671] [G_A loss: 0.563532, G_B loss: 1.341562]\n",
      "[Epoch 0/100] [Batch 115/1463] [D_A loss: 1.443998] [D_B loss: 1.537893] [G_A loss: 1.223707, G_B loss: 0.545882]\n",
      "[Epoch 0/100] [Batch 116/1463] [D_A loss: 1.404257] [D_B loss: 1.341018] [G_A loss: 0.878402, G_B loss: 1.393828]\n",
      "[Epoch 0/100] [Batch 117/1463] [D_A loss: 1.304012] [D_B loss: 1.476635] [G_A loss: 1.246081, G_B loss: 0.586889]\n",
      "[Epoch 0/100] [Batch 118/1463] [D_A loss: 1.275796] [D_B loss: 1.312533] [G_A loss: 0.782544, G_B loss: 1.975283]\n",
      "[Epoch 0/100] [Batch 119/1463] [D_A loss: 1.617099] [D_B loss: 1.505459] [G_A loss: 0.889199, G_B loss: 0.380179]\n",
      "[Epoch 0/100] [Batch 120/1463] [D_A loss: 1.650543] [D_B loss: 1.667838] [G_A loss: 0.984191, G_B loss: 1.609543]\n",
      "[Epoch 0/100] [Batch 121/1463] [D_A loss: 1.612563] [D_B loss: 1.575695] [G_A loss: 1.521596, G_B loss: 0.425924]\n",
      "[Epoch 0/100] [Batch 122/1463] [D_A loss: 1.585596] [D_B loss: 1.529735] [G_A loss: 0.463310, G_B loss: 1.262602]\n",
      "[Epoch 0/100] [Batch 123/1463] [D_A loss: 1.427424] [D_B loss: 1.524877] [G_A loss: 1.407649, G_B loss: 0.620575]\n",
      "[Epoch 0/100] [Batch 124/1463] [D_A loss: 1.444681] [D_B loss: 1.606606] [G_A loss: 0.473749, G_B loss: 0.961155]\n",
      "[Epoch 0/100] [Batch 125/1463] [D_A loss: 1.371307] [D_B loss: 1.619496] [G_A loss: 1.186888, G_B loss: 0.928693]\n",
      "[Epoch 0/100] [Batch 126/1463] [D_A loss: 1.309020] [D_B loss: 1.540913] [G_A loss: 0.730984, G_B loss: 1.005658]\n",
      "[Epoch 0/100] [Batch 127/1463] [D_A loss: 1.358772] [D_B loss: 1.536101] [G_A loss: 1.072603, G_B loss: 0.810290]\n",
      "[Epoch 0/100] [Batch 128/1463] [D_A loss: 1.334535] [D_B loss: 1.454387] [G_A loss: 0.736777, G_B loss: 1.474109]\n",
      "[Epoch 0/100] [Batch 129/1463] [D_A loss: 1.430389] [D_B loss: 1.429771] [G_A loss: 1.259989, G_B loss: 0.338119]\n",
      "[Epoch 0/100] [Batch 130/1463] [D_A loss: 1.650941] [D_B loss: 1.616191] [G_A loss: 0.609522, G_B loss: 1.633606]\n",
      "[Epoch 0/100] [Batch 131/1463] [D_A loss: 1.743035] [D_B loss: 1.574054] [G_A loss: 1.378480, G_B loss: 0.358221]\n",
      "[Epoch 0/100] [Batch 132/1463] [D_A loss: 1.573151] [D_B loss: 1.358575] [G_A loss: 0.857925, G_B loss: 1.408382]\n",
      "[Epoch 0/100] [Batch 133/1463] [D_A loss: 1.513906] [D_B loss: 1.560872] [G_A loss: 0.792177, G_B loss: 0.529975]\n",
      "[Epoch 0/100] [Batch 134/1463] [D_A loss: 1.501200] [D_B loss: 1.490839] [G_A loss: 1.081378, G_B loss: 1.160581]\n",
      "[Epoch 0/100] [Batch 135/1463] [D_A loss: 1.400806] [D_B loss: 1.613660] [G_A loss: 0.727068, G_B loss: 0.783190]\n",
      "[Epoch 0/100] [Batch 136/1463] [D_A loss: 1.491782] [D_B loss: 1.474794] [G_A loss: 1.080822, G_B loss: 0.807364]\n",
      "[Epoch 0/100] [Batch 137/1463] [D_A loss: 1.484179] [D_B loss: 1.502576] [G_A loss: 0.820155, G_B loss: 0.795029]\n",
      "[Epoch 0/100] [Batch 138/1463] [D_A loss: 1.409575] [D_B loss: 1.423541] [G_A loss: 1.055223, G_B loss: 0.895426]\n",
      "[Epoch 0/100] [Batch 139/1463] [D_A loss: 1.478054] [D_B loss: 1.552126] [G_A loss: 0.671115, G_B loss: 0.694202]\n",
      "[Epoch 0/100] [Batch 140/1463] [D_A loss: 1.423493] [D_B loss: 1.516183] [G_A loss: 1.070063, G_B loss: 1.027618]\n",
      "[Epoch 0/100] [Batch 141/1463] [D_A loss: 1.397624] [D_B loss: 1.524271] [G_A loss: 0.643451, G_B loss: 0.855107]\n",
      "[Epoch 0/100] [Batch 142/1463] [D_A loss: 1.491391] [D_B loss: 1.542649] [G_A loss: 1.090795, G_B loss: 0.627805]\n",
      "[Epoch 0/100] [Batch 143/1463] [D_A loss: 1.517012] [D_B loss: 1.512377] [G_A loss: 0.679025, G_B loss: 1.313551]\n",
      "[Epoch 0/100] [Batch 144/1463] [D_A loss: 1.450665] [D_B loss: 1.481623] [G_A loss: 0.976727, G_B loss: 0.551962]\n",
      "[Epoch 0/100] [Batch 145/1463] [D_A loss: 1.512693] [D_B loss: 1.446535] [G_A loss: 0.729821, G_B loss: 1.219178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 146/1463] [D_A loss: 1.491847] [D_B loss: 1.443757] [G_A loss: 0.895341, G_B loss: 0.513971]\n",
      "[Epoch 0/100] [Batch 147/1463] [D_A loss: 1.467202] [D_B loss: 1.506407] [G_A loss: 0.742209, G_B loss: 1.255419]\n",
      "[Epoch 0/100] [Batch 148/1463] [D_A loss: 1.472593] [D_B loss: 1.484068] [G_A loss: 0.943037, G_B loss: 0.598627]\n",
      "[Epoch 0/100] [Batch 149/1463] [D_A loss: 1.488975] [D_B loss: 1.549402] [G_A loss: 0.732039, G_B loss: 0.889892]\n",
      "[Epoch 0/100] [Batch 150/1463] [D_A loss: 1.406123] [D_B loss: 1.450393] [G_A loss: 0.895989, G_B loss: 0.845586]\n",
      "[Epoch 0/100] [Batch 151/1463] [D_A loss: 1.452981] [D_B loss: 1.441748] [G_A loss: 0.812508, G_B loss: 0.802523]\n",
      "[Epoch 0/100] [Batch 152/1463] [D_A loss: 1.421034] [D_B loss: 1.440423] [G_A loss: 0.939108, G_B loss: 1.135216]\n",
      "[Epoch 0/100] [Batch 153/1463] [D_A loss: 1.469645] [D_B loss: 1.471168] [G_A loss: 0.751364, G_B loss: 0.473906]\n",
      "[Epoch 0/100] [Batch 154/1463] [D_A loss: 1.492146] [D_B loss: 1.416489] [G_A loss: 1.032645, G_B loss: 1.284931]\n",
      "[Epoch 0/100] [Batch 155/1463] [D_A loss: 1.483294] [D_B loss: 1.430020] [G_A loss: 0.819290, G_B loss: 0.591851]\n",
      "[Epoch 0/100] [Batch 156/1463] [D_A loss: 1.462695] [D_B loss: 1.456604] [G_A loss: 0.912654, G_B loss: 1.040686]\n",
      "[Epoch 0/100] [Batch 157/1463] [D_A loss: 1.406177] [D_B loss: 1.412581] [G_A loss: 0.837743, G_B loss: 0.676822]\n",
      "[Epoch 0/100] [Batch 158/1463] [D_A loss: 1.440229] [D_B loss: 1.472940] [G_A loss: 0.834566, G_B loss: 1.055992]\n",
      "[Epoch 0/100] [Batch 159/1463] [D_A loss: 1.447013] [D_B loss: 1.486731] [G_A loss: 0.767947, G_B loss: 0.689757]\n",
      "[Epoch 0/100] [Batch 160/1463] [D_A loss: 1.454966] [D_B loss: 1.463161] [G_A loss: 0.872034, G_B loss: 0.942008]\n",
      "[Epoch 0/100] [Batch 161/1463] [D_A loss: 1.505077] [D_B loss: 1.466835] [G_A loss: 0.740608, G_B loss: 0.624757]\n",
      "[Epoch 0/100] [Batch 162/1463] [D_A loss: 1.451478] [D_B loss: 1.436138] [G_A loss: 0.857170, G_B loss: 1.092057]\n",
      "[Epoch 0/100] [Batch 163/1463] [D_A loss: 1.480307] [D_B loss: 1.505229] [G_A loss: 0.777405, G_B loss: 0.579437]\n",
      "[Epoch 0/100] [Batch 164/1463] [D_A loss: 1.451276] [D_B loss: 1.477365] [G_A loss: 0.915337, G_B loss: 0.997266]\n",
      "[Epoch 0/100] [Batch 165/1463] [D_A loss: 1.477447] [D_B loss: 1.451589] [G_A loss: 0.742236, G_B loss: 0.709110]\n",
      "[Epoch 0/100] [Batch 166/1463] [D_A loss: 1.496192] [D_B loss: 1.508274] [G_A loss: 0.802405, G_B loss: 0.974874]\n",
      "[Epoch 0/100] [Batch 167/1463] [D_A loss: 1.433695] [D_B loss: 1.436234] [G_A loss: 0.758670, G_B loss: 0.601272]\n",
      "[Epoch 0/100] [Batch 168/1463] [D_A loss: 1.466361] [D_B loss: 1.458299] [G_A loss: 0.802267, G_B loss: 1.018739]\n",
      "[Epoch 0/100] [Batch 169/1463] [D_A loss: 1.504399] [D_B loss: 1.420609] [G_A loss: 0.877141, G_B loss: 0.570010]\n",
      "[Epoch 0/100] [Batch 170/1463] [D_A loss: 1.478740] [D_B loss: 1.467737] [G_A loss: 0.661054, G_B loss: 0.887881]\n",
      "[Epoch 0/100] [Batch 171/1463] [D_A loss: 1.445610] [D_B loss: 1.436023] [G_A loss: 1.050948, G_B loss: 0.732375]\n",
      "[Epoch 0/100] [Batch 172/1463] [D_A loss: 1.412038] [D_B loss: 1.407868] [G_A loss: 0.807008, G_B loss: 0.892505]\n",
      "[Epoch 0/100] [Batch 173/1463] [D_A loss: 1.440583] [D_B loss: 1.440663] [G_A loss: 0.858876, G_B loss: 0.679013]\n",
      "[Epoch 0/100] [Batch 174/1463] [D_A loss: 1.462089] [D_B loss: 1.444310] [G_A loss: 0.755227, G_B loss: 0.859961]\n",
      "[Epoch 0/100] [Batch 175/1463] [D_A loss: 1.461519] [D_B loss: 1.414210] [G_A loss: 0.905580, G_B loss: 0.719044]\n",
      "[Epoch 0/100] [Batch 176/1463] [D_A loss: 1.430019] [D_B loss: 1.442291] [G_A loss: 0.657553, G_B loss: 0.882020]\n",
      "[Epoch 0/100] [Batch 177/1463] [D_A loss: 1.433539] [D_B loss: 1.501224] [G_A loss: 0.921364, G_B loss: 0.698721]\n",
      "[Epoch 0/100] [Batch 178/1463] [D_A loss: 1.394983] [D_B loss: 1.447785] [G_A loss: 0.736722, G_B loss: 0.839630]\n",
      "[Epoch 0/100] [Batch 179/1463] [D_A loss: 1.446363] [D_B loss: 1.437720] [G_A loss: 0.864538, G_B loss: 0.724111]\n",
      "[Epoch 0/100] [Batch 180/1463] [D_A loss: 1.440052] [D_B loss: 1.453382] [G_A loss: 0.848310, G_B loss: 0.827610]\n",
      "[Epoch 0/100] [Batch 181/1463] [D_A loss: 1.432009] [D_B loss: 1.425115] [G_A loss: 0.838148, G_B loss: 0.742230]\n",
      "[Epoch 0/100] [Batch 182/1463] [D_A loss: 1.395989] [D_B loss: 1.401928] [G_A loss: 0.856372, G_B loss: 0.811459]\n",
      "[Epoch 0/100] [Batch 183/1463] [D_A loss: 1.411241] [D_B loss: 1.426370] [G_A loss: 0.772741, G_B loss: 0.762019]\n",
      "[Epoch 0/100] [Batch 184/1463] [D_A loss: 1.436196] [D_B loss: 1.466819] [G_A loss: 0.757448, G_B loss: 0.807905]\n",
      "[Epoch 0/100] [Batch 185/1463] [D_A loss: 1.413090] [D_B loss: 1.421194] [G_A loss: 0.954030, G_B loss: 0.684414]\n",
      "[Epoch 0/100] [Batch 186/1463] [D_A loss: 1.467272] [D_B loss: 1.463451] [G_A loss: 0.685150, G_B loss: 0.911746]\n",
      "[Epoch 0/100] [Batch 187/1463] [D_A loss: 1.502165] [D_B loss: 1.421082] [G_A loss: 0.912804, G_B loss: 0.806586]\n",
      "[Epoch 0/100] [Batch 188/1463] [D_A loss: 1.422229] [D_B loss: 1.448019] [G_A loss: 0.678473, G_B loss: 0.926958]\n",
      "[Epoch 0/100] [Batch 189/1463] [D_A loss: 1.460009] [D_B loss: 1.446105] [G_A loss: 0.871650, G_B loss: 0.588441]\n",
      "[Epoch 0/100] [Batch 190/1463] [D_A loss: 1.483316] [D_B loss: 1.372377] [G_A loss: 0.756397, G_B loss: 0.922093]\n",
      "[Epoch 0/100] [Batch 191/1463] [D_A loss: 1.452394] [D_B loss: 1.447396] [G_A loss: 0.846724, G_B loss: 0.700355]\n",
      "[Epoch 0/100] [Batch 192/1463] [D_A loss: 1.423896] [D_B loss: 1.425458] [G_A loss: 0.686621, G_B loss: 0.880355]\n",
      "[Epoch 0/100] [Batch 193/1463] [D_A loss: 1.440413] [D_B loss: 1.453747] [G_A loss: 0.888593, G_B loss: 0.680955]\n",
      "[Epoch 0/100] [Batch 194/1463] [D_A loss: 1.409254] [D_B loss: 1.439443] [G_A loss: 0.699999, G_B loss: 0.841959]\n",
      "[Epoch 0/100] [Batch 195/1463] [D_A loss: 1.409059] [D_B loss: 1.472133] [G_A loss: 0.979870, G_B loss: 0.779852]\n",
      "[Epoch 0/100] [Batch 196/1463] [D_A loss: 1.424517] [D_B loss: 1.455774] [G_A loss: 0.719317, G_B loss: 0.687905]\n",
      "[Epoch 0/100] [Batch 197/1463] [D_A loss: 1.442959] [D_B loss: 1.421359] [G_A loss: 0.830072, G_B loss: 0.864568]\n",
      "[Epoch 0/100] [Batch 198/1463] [D_A loss: 1.451631] [D_B loss: 1.391249] [G_A loss: 0.713580, G_B loss: 0.635206]\n",
      "[Epoch 0/100] [Batch 199/1463] [D_A loss: 1.423064] [D_B loss: 1.429264] [G_A loss: 0.876147, G_B loss: 0.934798]\n",
      "[Epoch 0/100] [Batch 200/1463] [D_A loss: 1.429543] [D_B loss: 1.442007] [G_A loss: 0.764793, G_B loss: 0.613617]\n",
      "[Epoch 0/100] [Batch 201/1463] [D_A loss: 1.444429] [D_B loss: 1.417563] [G_A loss: 0.830652, G_B loss: 0.903407]\n",
      "[Epoch 0/100] [Batch 202/1463] [D_A loss: 1.410145] [D_B loss: 1.428863] [G_A loss: 0.799337, G_B loss: 0.662582]\n",
      "[Epoch 0/100] [Batch 203/1463] [D_A loss: 1.403906] [D_B loss: 1.408406] [G_A loss: 0.842182, G_B loss: 0.860424]\n",
      "[Epoch 0/100] [Batch 204/1463] [D_A loss: 1.437694] [D_B loss: 1.435476] [G_A loss: 0.661588, G_B loss: 0.735317]\n",
      "[Epoch 0/100] [Batch 205/1463] [D_A loss: 1.406900] [D_B loss: 1.447610] [G_A loss: 0.867315, G_B loss: 0.783216]\n",
      "[Epoch 0/100] [Batch 206/1463] [D_A loss: 1.382401] [D_B loss: 1.421525] [G_A loss: 0.798931, G_B loss: 0.780884]\n",
      "[Epoch 0/100] [Batch 207/1463] [D_A loss: 1.408569] [D_B loss: 1.408231] [G_A loss: 0.842111, G_B loss: 0.833079]\n",
      "[Epoch 0/100] [Batch 208/1463] [D_A loss: 1.442430] [D_B loss: 1.423668] [G_A loss: 0.714469, G_B loss: 0.644199]\n",
      "[Epoch 0/100] [Batch 209/1463] [D_A loss: 1.469852] [D_B loss: 1.447739] [G_A loss: 0.929160, G_B loss: 1.034296]\n",
      "[Epoch 0/100] [Batch 210/1463] [D_A loss: 1.446751] [D_B loss: 1.431053] [G_A loss: 0.643686, G_B loss: 0.656902]\n",
      "[Epoch 0/100] [Batch 211/1463] [D_A loss: 1.408112] [D_B loss: 1.403068] [G_A loss: 0.886229, G_B loss: 0.887870]\n",
      "[Epoch 0/100] [Batch 212/1463] [D_A loss: 1.450734] [D_B loss: 1.435309] [G_A loss: 0.698637, G_B loss: 0.729022]\n",
      "[Epoch 0/100] [Batch 213/1463] [D_A loss: 1.418730] [D_B loss: 1.438030] [G_A loss: 0.827415, G_B loss: 0.780583]\n",
      "[Epoch 0/100] [Batch 214/1463] [D_A loss: 1.392138] [D_B loss: 1.449096] [G_A loss: 0.730226, G_B loss: 0.727435]\n",
      "[Epoch 0/100] [Batch 215/1463] [D_A loss: 1.411728] [D_B loss: 1.427793] [G_A loss: 0.759715, G_B loss: 0.780775]\n",
      "[Epoch 0/100] [Batch 216/1463] [D_A loss: 1.417161] [D_B loss: 1.403463] [G_A loss: 0.784922, G_B loss: 0.676890]\n",
      "[Epoch 0/100] [Batch 217/1463] [D_A loss: 1.439587] [D_B loss: 1.412699] [G_A loss: 0.709868, G_B loss: 0.855606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 218/1463] [D_A loss: 1.434362] [D_B loss: 1.439059] [G_A loss: 0.856337, G_B loss: 0.625369]\n",
      "[Epoch 0/100] [Batch 219/1463] [D_A loss: 1.469218] [D_B loss: 1.444087] [G_A loss: 0.727650, G_B loss: 0.915859]\n",
      "[Epoch 0/100] [Batch 220/1463] [D_A loss: 1.525594] [D_B loss: 1.441754] [G_A loss: 0.833899, G_B loss: 0.841225]\n",
      "[Epoch 0/100] [Batch 221/1463] [D_A loss: 1.396624] [D_B loss: 1.423615] [G_A loss: 0.745115, G_B loss: 0.763644]\n",
      "[Epoch 0/100] [Batch 222/1463] [D_A loss: 1.398333] [D_B loss: 1.425818] [G_A loss: 0.757420, G_B loss: 0.732383]\n",
      "[Epoch 0/100] [Batch 223/1463] [D_A loss: 1.427708] [D_B loss: 1.415908] [G_A loss: 0.848170, G_B loss: 0.745645]\n",
      "[Epoch 0/100] [Batch 224/1463] [D_A loss: 1.413277] [D_B loss: 1.410570] [G_A loss: 0.699876, G_B loss: 0.799231]\n",
      "[Epoch 0/100] [Batch 225/1463] [D_A loss: 1.425462] [D_B loss: 1.452962] [G_A loss: 0.939777, G_B loss: 0.680230]\n",
      "[Epoch 0/100] [Batch 226/1463] [D_A loss: 1.408016] [D_B loss: 1.434764] [G_A loss: 0.720917, G_B loss: 0.828271]\n",
      "[Epoch 0/100] [Batch 227/1463] [D_A loss: 1.415449] [D_B loss: 1.433717] [G_A loss: 0.788231, G_B loss: 0.629621]\n",
      "[Epoch 0/100] [Batch 228/1463] [D_A loss: 1.418540] [D_B loss: 1.410947] [G_A loss: 0.702704, G_B loss: 0.870334]\n",
      "[Epoch 0/100] [Batch 229/1463] [D_A loss: 1.409798] [D_B loss: 1.421089] [G_A loss: 0.832302, G_B loss: 0.669191]\n",
      "[Epoch 0/100] [Batch 230/1463] [D_A loss: 1.411494] [D_B loss: 1.402091] [G_A loss: 0.738364, G_B loss: 0.852756]\n",
      "[Epoch 0/100] [Batch 231/1463] [D_A loss: 1.407761] [D_B loss: 1.421157] [G_A loss: 0.820847, G_B loss: 0.615903]\n",
      "[Epoch 0/100] [Batch 232/1463] [D_A loss: 1.433776] [D_B loss: 1.405692] [G_A loss: 0.717027, G_B loss: 0.908914]\n",
      "[Epoch 0/100] [Batch 233/1463] [D_A loss: 1.431216] [D_B loss: 1.403482] [G_A loss: 0.849014, G_B loss: 0.604931]\n",
      "[Epoch 0/100] [Batch 234/1463] [D_A loss: 1.425483] [D_B loss: 1.432137] [G_A loss: 0.688761, G_B loss: 0.803006]\n",
      "[Epoch 0/100] [Batch 235/1463] [D_A loss: 1.408001] [D_B loss: 1.426784] [G_A loss: 0.874316, G_B loss: 0.664592]\n",
      "[Epoch 0/100] [Batch 236/1463] [D_A loss: 1.389928] [D_B loss: 1.402234] [G_A loss: 0.703334, G_B loss: 0.790152]\n",
      "[Epoch 0/100] [Batch 237/1463] [D_A loss: 1.409568] [D_B loss: 1.430571] [G_A loss: 0.833488, G_B loss: 0.675013]\n",
      "[Epoch 0/100] [Batch 238/1463] [D_A loss: 1.405848] [D_B loss: 1.403678] [G_A loss: 0.663115, G_B loss: 0.835108]\n",
      "[Epoch 0/100] [Batch 239/1463] [D_A loss: 1.407825] [D_B loss: 1.407207] [G_A loss: 0.836622, G_B loss: 0.668647]\n",
      "[Epoch 0/100] [Batch 240/1463] [D_A loss: 1.401542] [D_B loss: 1.427164] [G_A loss: 0.652236, G_B loss: 0.792246]\n",
      "[Epoch 0/100] [Batch 241/1463] [D_A loss: 1.417855] [D_B loss: 1.445012] [G_A loss: 0.854791, G_B loss: 0.672736]\n",
      "[Epoch 0/100] [Batch 242/1463] [D_A loss: 1.437010] [D_B loss: 1.442958] [G_A loss: 0.710036, G_B loss: 0.872297]\n",
      "[Epoch 0/100] [Batch 243/1463] [D_A loss: 1.407588] [D_B loss: 1.415838] [G_A loss: 0.810528, G_B loss: 0.602075]\n",
      "[Epoch 0/100] [Batch 244/1463] [D_A loss: 1.433050] [D_B loss: 1.421128] [G_A loss: 0.695981, G_B loss: 0.864696]\n",
      "[Epoch 0/100] [Batch 245/1463] [D_A loss: 1.421793] [D_B loss: 1.427354] [G_A loss: 0.801840, G_B loss: 0.607243]\n",
      "[Epoch 0/100] [Batch 246/1463] [D_A loss: 1.415027] [D_B loss: 1.423741] [G_A loss: 0.672605, G_B loss: 0.852551]\n",
      "[Epoch 0/100] [Batch 247/1463] [D_A loss: 1.429579] [D_B loss: 1.429839] [G_A loss: 0.815120, G_B loss: 0.670816]\n",
      "[Epoch 0/100] [Batch 248/1463] [D_A loss: 1.416290] [D_B loss: 1.422785] [G_A loss: 0.727660, G_B loss: 0.764108]\n",
      "[Epoch 0/100] [Batch 249/1463] [D_A loss: 1.406387] [D_B loss: 1.410511] [G_A loss: 0.792407, G_B loss: 0.678777]\n",
      "[Epoch 0/100] [Batch 250/1463] [D_A loss: 1.410561] [D_B loss: 1.405684] [G_A loss: 0.679283, G_B loss: 0.741478]\n",
      "[Epoch 0/100] [Batch 251/1463] [D_A loss: 1.399381] [D_B loss: 1.402533] [G_A loss: 0.810518, G_B loss: 0.735678]\n",
      "[Epoch 0/100] [Batch 252/1463] [D_A loss: 1.406444] [D_B loss: 1.419086] [G_A loss: 0.710434, G_B loss: 0.703749]\n",
      "[Epoch 0/100] [Batch 253/1463] [D_A loss: 1.404760] [D_B loss: 1.420773] [G_A loss: 0.808594, G_B loss: 0.763467]\n",
      "[Epoch 0/100] [Batch 254/1463] [D_A loss: 1.416271] [D_B loss: 1.421962] [G_A loss: 0.662825, G_B loss: 0.689943]\n",
      "[Epoch 0/100] [Batch 255/1463] [D_A loss: 1.402315] [D_B loss: 1.435205] [G_A loss: 0.797731, G_B loss: 0.770625]\n",
      "[Epoch 0/100] [Batch 256/1463] [D_A loss: 1.396328] [D_B loss: 1.405992] [G_A loss: 0.727422, G_B loss: 0.688567]\n",
      "[Epoch 0/100] [Batch 257/1463] [D_A loss: 1.406607] [D_B loss: 1.424097] [G_A loss: 0.795698, G_B loss: 0.768402]\n",
      "[Epoch 0/100] [Batch 258/1463] [D_A loss: 1.398036] [D_B loss: 1.413422] [G_A loss: 0.686706, G_B loss: 0.649467]\n",
      "[Epoch 0/100] [Batch 259/1463] [D_A loss: 1.411358] [D_B loss: 1.412971] [G_A loss: 0.810062, G_B loss: 0.858161]\n",
      "[Epoch 0/100] [Batch 260/1463] [D_A loss: 1.434553] [D_B loss: 1.416955] [G_A loss: 0.717815, G_B loss: 0.665717]\n",
      "[Epoch 0/100] [Batch 261/1463] [D_A loss: 1.445521] [D_B loss: 1.443891] [G_A loss: 0.847126, G_B loss: 0.878917]\n",
      "[Epoch 0/100] [Batch 262/1463] [D_A loss: 1.417648] [D_B loss: 1.402105] [G_A loss: 0.726009, G_B loss: 0.625233]\n",
      "[Epoch 0/100] [Batch 263/1463] [D_A loss: 1.393938] [D_B loss: 1.415779] [G_A loss: 0.698673, G_B loss: 0.823668]\n",
      "[Epoch 0/100] [Batch 264/1463] [D_A loss: 1.426039] [D_B loss: 1.405983] [G_A loss: 0.795389, G_B loss: 0.737246]\n",
      "[Epoch 0/100] [Batch 265/1463] [D_A loss: 1.413766] [D_B loss: 1.427124] [G_A loss: 0.656499, G_B loss: 0.715820]\n",
      "[Epoch 0/100] [Batch 266/1463] [D_A loss: 1.408383] [D_B loss: 1.426690] [G_A loss: 0.802963, G_B loss: 0.747396]\n",
      "[Epoch 0/100] [Batch 267/1463] [D_A loss: 1.398411] [D_B loss: 1.407754] [G_A loss: 0.705570, G_B loss: 0.733348]\n",
      "[Epoch 0/100] [Batch 268/1463] [D_A loss: 1.420327] [D_B loss: 1.394106] [G_A loss: 0.782902, G_B loss: 0.792975]\n",
      "[Epoch 0/100] [Batch 269/1463] [D_A loss: 1.408750] [D_B loss: 1.390619] [G_A loss: 0.705113, G_B loss: 0.710471]\n",
      "[Epoch 0/100] [Batch 270/1463] [D_A loss: 1.442618] [D_B loss: 1.431214] [G_A loss: 0.781482, G_B loss: 0.838186]\n",
      "[Epoch 0/100] [Batch 271/1463] [D_A loss: 1.397109] [D_B loss: 1.416750] [G_A loss: 0.696528, G_B loss: 0.670415]\n",
      "[Epoch 0/100] [Batch 272/1463] [D_A loss: 1.403696] [D_B loss: 1.424008] [G_A loss: 0.756662, G_B loss: 0.728597]\n",
      "[Epoch 0/100] [Batch 273/1463] [D_A loss: 1.408004] [D_B loss: 1.421095] [G_A loss: 0.756827, G_B loss: 0.734256]\n",
      "[Epoch 0/100] [Batch 274/1463] [D_A loss: 1.409095] [D_B loss: 1.403593] [G_A loss: 0.733312, G_B loss: 0.766484]\n",
      "[Epoch 0/100] [Batch 275/1463] [D_A loss: 1.400517] [D_B loss: 1.393643] [G_A loss: 0.737253, G_B loss: 0.655812]\n",
      "[Epoch 0/100] [Batch 276/1463] [D_A loss: 1.407534] [D_B loss: 1.420066] [G_A loss: 0.729547, G_B loss: 0.819123]\n",
      "[Epoch 0/100] [Batch 277/1463] [D_A loss: 1.404033] [D_B loss: 1.403347] [G_A loss: 0.759364, G_B loss: 0.654600]\n",
      "[Epoch 0/100] [Batch 278/1463] [D_A loss: 1.413959] [D_B loss: 1.406715] [G_A loss: 0.686691, G_B loss: 0.790177]\n",
      "[Epoch 0/100] [Batch 279/1463] [D_A loss: 1.421122] [D_B loss: 1.413682] [G_A loss: 0.794077, G_B loss: 0.677244]\n",
      "[Epoch 0/100] [Batch 280/1463] [D_A loss: 1.416967] [D_B loss: 1.411156] [G_A loss: 0.645203, G_B loss: 0.816164]\n",
      "[Epoch 0/100] [Batch 281/1463] [D_A loss: 1.408197] [D_B loss: 1.413702] [G_A loss: 0.880713, G_B loss: 0.615459]\n",
      "[Epoch 0/100] [Batch 282/1463] [D_A loss: 1.408837] [D_B loss: 1.428999] [G_A loss: 0.628367, G_B loss: 0.848588]\n",
      "[Epoch 0/100] [Batch 283/1463] [D_A loss: 1.410806] [D_B loss: 1.429202] [G_A loss: 0.869013, G_B loss: 0.631281]\n",
      "[Epoch 0/100] [Batch 284/1463] [D_A loss: 1.408974] [D_B loss: 1.426264] [G_A loss: 0.663551, G_B loss: 0.799678]\n",
      "[Epoch 0/100] [Batch 285/1463] [D_A loss: 1.403131] [D_B loss: 1.434459] [G_A loss: 0.786591, G_B loss: 0.669456]\n",
      "[Epoch 0/100] [Batch 286/1463] [D_A loss: 1.410322] [D_B loss: 1.421995] [G_A loss: 0.703716, G_B loss: 0.752562]\n",
      "[Epoch 0/100] [Batch 287/1463] [D_A loss: 1.402147] [D_B loss: 1.407042] [G_A loss: 0.723244, G_B loss: 0.702840]\n",
      "[Epoch 0/100] [Batch 288/1463] [D_A loss: 1.392327] [D_B loss: 1.408508] [G_A loss: 0.738397, G_B loss: 0.748953]\n",
      "[Epoch 0/100] [Batch 289/1463] [D_A loss: 1.411008] [D_B loss: 1.415476] [G_A loss: 0.702323, G_B loss: 0.715209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 290/1463] [D_A loss: 1.406568] [D_B loss: 1.408033] [G_A loss: 0.745443, G_B loss: 0.693636]\n",
      "[Epoch 0/100] [Batch 291/1463] [D_A loss: 1.405096] [D_B loss: 1.411905] [G_A loss: 0.739841, G_B loss: 0.738772]\n",
      "[Epoch 0/100] [Batch 292/1463] [D_A loss: 1.404773] [D_B loss: 1.405772] [G_A loss: 0.730675, G_B loss: 0.684181]\n",
      "[Epoch 0/100] [Batch 293/1463] [D_A loss: 1.398589] [D_B loss: 1.405707] [G_A loss: 0.695954, G_B loss: 0.824222]\n",
      "[Epoch 0/100] [Batch 294/1463] [D_A loss: 1.407291] [D_B loss: 1.411144] [G_A loss: 0.754372, G_B loss: 0.655458]\n",
      "[Epoch 0/100] [Batch 295/1463] [D_A loss: 1.420009] [D_B loss: 1.399875] [G_A loss: 0.731598, G_B loss: 0.835952]\n",
      "[Epoch 0/100] [Batch 296/1463] [D_A loss: 1.417316] [D_B loss: 1.405336] [G_A loss: 0.707501, G_B loss: 0.670478]\n",
      "[Epoch 0/100] [Batch 297/1463] [D_A loss: 1.423547] [D_B loss: 1.423031] [G_A loss: 0.808645, G_B loss: 0.770859]\n",
      "[Epoch 0/100] [Batch 298/1463] [D_A loss: 1.399212] [D_B loss: 1.404583] [G_A loss: 0.725205, G_B loss: 0.667305]\n",
      "[Epoch 0/100] [Batch 299/1463] [D_A loss: 1.411349] [D_B loss: 1.405111] [G_A loss: 0.720907, G_B loss: 0.792716]\n",
      "[Epoch 0/100] [Batch 300/1463] [D_A loss: 1.408540] [D_B loss: 1.393398] [G_A loss: 0.726804, G_B loss: 0.674817]\n",
      "[Epoch 0/100] [Batch 301/1463] [D_A loss: 1.396691] [D_B loss: 1.384866] [G_A loss: 0.696079, G_B loss: 0.749516]\n",
      "[Epoch 0/100] [Batch 302/1463] [D_A loss: 1.394696] [D_B loss: 1.407728] [G_A loss: 0.825599, G_B loss: 0.669924]\n",
      "[Epoch 0/100] [Batch 303/1463] [D_A loss: 1.400735] [D_B loss: 1.408675] [G_A loss: 0.664311, G_B loss: 0.796327]\n",
      "[Epoch 0/100] [Batch 304/1463] [D_A loss: 1.413017] [D_B loss: 1.403043] [G_A loss: 0.791163, G_B loss: 0.650540]\n",
      "[Epoch 0/100] [Batch 305/1463] [D_A loss: 1.410340] [D_B loss: 1.399504] [G_A loss: 0.715725, G_B loss: 0.784492]\n",
      "[Epoch 0/100] [Batch 306/1463] [D_A loss: 1.427767] [D_B loss: 1.412538] [G_A loss: 0.704853, G_B loss: 0.678858]\n",
      "[Epoch 0/100] [Batch 307/1463] [D_A loss: 1.405862] [D_B loss: 1.416373] [G_A loss: 0.740325, G_B loss: 0.759535]\n",
      "[Epoch 0/100] [Batch 308/1463] [D_A loss: 1.404741] [D_B loss: 1.405895] [G_A loss: 0.734049, G_B loss: 0.658179]\n",
      "[Epoch 0/100] [Batch 309/1463] [D_A loss: 1.404326] [D_B loss: 1.400192] [G_A loss: 0.686224, G_B loss: 0.775965]\n",
      "[Epoch 0/100] [Batch 310/1463] [D_A loss: 1.401709] [D_B loss: 1.397015] [G_A loss: 0.781945, G_B loss: 0.633528]\n",
      "[Epoch 0/100] [Batch 311/1463] [D_A loss: 1.402648] [D_B loss: 1.403957] [G_A loss: 0.651396, G_B loss: 0.786782]\n",
      "[Epoch 0/100] [Batch 312/1463] [D_A loss: 1.400128] [D_B loss: 1.401899] [G_A loss: 0.799313, G_B loss: 0.650639]\n",
      "[Epoch 0/100] [Batch 313/1463] [D_A loss: 1.429651] [D_B loss: 1.405656] [G_A loss: 0.628847, G_B loss: 0.791623]\n",
      "[Epoch 0/100] [Batch 314/1463] [D_A loss: 1.397441] [D_B loss: 1.404638] [G_A loss: 0.819264, G_B loss: 0.672056]\n",
      "[Epoch 0/100] [Batch 315/1463] [D_A loss: 1.401589] [D_B loss: 1.402835] [G_A loss: 0.630793, G_B loss: 0.753679]\n",
      "[Epoch 0/100] [Batch 316/1463] [D_A loss: 1.384300] [D_B loss: 1.406359] [G_A loss: 0.806760, G_B loss: 0.673887]\n",
      "[Epoch 0/100] [Batch 317/1463] [D_A loss: 1.392312] [D_B loss: 1.406255] [G_A loss: 0.673659, G_B loss: 0.772588]\n",
      "[Epoch 0/100] [Batch 318/1463] [D_A loss: 1.411339] [D_B loss: 1.418987] [G_A loss: 0.774436, G_B loss: 0.682826]\n",
      "[Epoch 0/100] [Batch 319/1463] [D_A loss: 1.395548] [D_B loss: 1.402542] [G_A loss: 0.676140, G_B loss: 0.742583]\n",
      "[Epoch 0/100] [Batch 320/1463] [D_A loss: 1.395844] [D_B loss: 1.408164] [G_A loss: 0.767800, G_B loss: 0.733668]\n",
      "[Epoch 0/100] [Batch 321/1463] [D_A loss: 1.390927] [D_B loss: 1.404607] [G_A loss: 0.691881, G_B loss: 0.692624]\n",
      "[Epoch 0/100] [Batch 322/1463] [D_A loss: 1.397885] [D_B loss: 1.402773] [G_A loss: 0.761869, G_B loss: 0.726219]\n",
      "[Epoch 0/100] [Batch 323/1463] [D_A loss: 1.407473] [D_B loss: 1.415836] [G_A loss: 0.642919, G_B loss: 0.706449]\n",
      "[Epoch 0/100] [Batch 324/1463] [D_A loss: 1.395400] [D_B loss: 1.409814] [G_A loss: 0.816554, G_B loss: 0.744446]\n",
      "[Epoch 0/100] [Batch 325/1463] [D_A loss: 1.393622] [D_B loss: 1.417991] [G_A loss: 0.683746, G_B loss: 0.686597]\n",
      "[Epoch 0/100] [Batch 326/1463] [D_A loss: 1.396178] [D_B loss: 1.399044] [G_A loss: 0.755931, G_B loss: 0.770152]\n",
      "[Epoch 0/100] [Batch 327/1463] [D_A loss: 1.398535] [D_B loss: 1.400165] [G_A loss: 0.684190, G_B loss: 0.633364]\n",
      "[Epoch 0/100] [Batch 328/1463] [D_A loss: 1.402606] [D_B loss: 1.404732] [G_A loss: 0.749487, G_B loss: 0.825596]\n",
      "[Epoch 0/100] [Batch 329/1463] [D_A loss: 1.419019] [D_B loss: 1.399397] [G_A loss: 0.671042, G_B loss: 0.583110]\n",
      "[Epoch 0/100] [Batch 330/1463] [D_A loss: 1.408251] [D_B loss: 1.395958] [G_A loss: 0.770137, G_B loss: 0.837978]\n",
      "[Epoch 0/100] [Batch 331/1463] [D_A loss: 1.415641] [D_B loss: 1.415263] [G_A loss: 0.696784, G_B loss: 0.636706]\n",
      "[Epoch 0/100] [Batch 332/1463] [D_A loss: 1.404680] [D_B loss: 1.406643] [G_A loss: 0.768433, G_B loss: 0.759011]\n",
      "[Epoch 0/100] [Batch 333/1463] [D_A loss: 1.411762] [D_B loss: 1.401719] [G_A loss: 0.681341, G_B loss: 0.688614]\n",
      "[Epoch 0/100] [Batch 334/1463] [D_A loss: 1.399634] [D_B loss: 1.399652] [G_A loss: 0.781266, G_B loss: 0.729972]\n",
      "[Epoch 0/100] [Batch 335/1463] [D_A loss: 1.404743] [D_B loss: 1.405576] [G_A loss: 0.651478, G_B loss: 0.727303]\n",
      "[Epoch 0/100] [Batch 336/1463] [D_A loss: 1.390917] [D_B loss: 1.402864] [G_A loss: 0.747689, G_B loss: 0.687647]\n",
      "[Epoch 0/100] [Batch 337/1463] [D_A loss: 1.390015] [D_B loss: 1.390543] [G_A loss: 0.695145, G_B loss: 0.743709]\n",
      "[Epoch 0/100] [Batch 338/1463] [D_A loss: 1.402033] [D_B loss: 1.400284] [G_A loss: 0.752622, G_B loss: 0.714102]\n",
      "[Epoch 0/100] [Batch 339/1463] [D_A loss: 1.393486] [D_B loss: 1.403834] [G_A loss: 0.676900, G_B loss: 0.704008]\n",
      "[Epoch 0/100] [Batch 340/1463] [D_A loss: 1.395767] [D_B loss: 1.410353] [G_A loss: 0.761974, G_B loss: 0.682509]\n",
      "[Epoch 0/100] [Batch 341/1463] [D_A loss: 1.397030] [D_B loss: 1.399692] [G_A loss: 0.671751, G_B loss: 0.760386]\n",
      "[Epoch 0/100] [Batch 342/1463] [D_A loss: 1.405766] [D_B loss: 1.406321] [G_A loss: 0.773919, G_B loss: 0.691351]\n",
      "[Epoch 0/100] [Batch 343/1463] [D_A loss: 1.403151] [D_B loss: 1.403062] [G_A loss: 0.667896, G_B loss: 0.750730]\n",
      "[Epoch 0/100] [Batch 344/1463] [D_A loss: 1.396321] [D_B loss: 1.400996] [G_A loss: 0.760783, G_B loss: 0.665485]\n",
      "[Epoch 0/100] [Batch 345/1463] [D_A loss: 1.409713] [D_B loss: 1.402847] [G_A loss: 0.664416, G_B loss: 0.763069]\n",
      "[Epoch 0/100] [Batch 346/1463] [D_A loss: 1.391966] [D_B loss: 1.393753] [G_A loss: 0.758184, G_B loss: 0.662358]\n",
      "[Epoch 0/100] [Batch 347/1463] [D_A loss: 1.411753] [D_B loss: 1.406998] [G_A loss: 0.655779, G_B loss: 0.817119]\n",
      "[Epoch 0/100] [Batch 348/1463] [D_A loss: 1.403128] [D_B loss: 1.400905] [G_A loss: 0.750190, G_B loss: 0.652872]\n",
      "[Epoch 0/100] [Batch 349/1463] [D_A loss: 1.396497] [D_B loss: 1.395675] [G_A loss: 0.678418, G_B loss: 0.800756]\n",
      "[Epoch 0/100] [Batch 350/1463] [D_A loss: 1.401166] [D_B loss: 1.406931] [G_A loss: 0.772408, G_B loss: 0.606518]\n",
      "[Epoch 0/100] [Batch 351/1463] [D_A loss: 1.413417] [D_B loss: 1.400967] [G_A loss: 0.693554, G_B loss: 0.795605]\n",
      "[Epoch 0/100] [Batch 352/1463] [D_A loss: 1.399113] [D_B loss: 1.399970] [G_A loss: 0.757927, G_B loss: 0.663756]\n",
      "[Epoch 0/100] [Batch 353/1463] [D_A loss: 1.403134] [D_B loss: 1.402096] [G_A loss: 0.662682, G_B loss: 0.766150]\n",
      "[Epoch 0/100] [Batch 354/1463] [D_A loss: 1.400028] [D_B loss: 1.401063] [G_A loss: 0.786168, G_B loss: 0.648256]\n",
      "[Epoch 0/100] [Batch 355/1463] [D_A loss: 1.398076] [D_B loss: 1.406980] [G_A loss: 0.638158, G_B loss: 0.751808]\n",
      "[Epoch 0/100] [Batch 356/1463] [D_A loss: 1.391759] [D_B loss: 1.410770] [G_A loss: 0.800695, G_B loss: 0.676073]\n",
      "[Epoch 0/100] [Batch 357/1463] [D_A loss: 1.403269] [D_B loss: 1.407350] [G_A loss: 0.641686, G_B loss: 0.740822]\n",
      "[Epoch 0/100] [Batch 358/1463] [D_A loss: 1.393238] [D_B loss: 1.401244] [G_A loss: 0.778021, G_B loss: 0.668751]\n",
      "[Epoch 0/100] [Batch 359/1463] [D_A loss: 1.395881] [D_B loss: 1.398021] [G_A loss: 0.659936, G_B loss: 0.729239]\n",
      "[Epoch 0/100] [Batch 360/1463] [D_A loss: 1.392097] [D_B loss: 1.398965] [G_A loss: 0.756347, G_B loss: 0.686306]\n"
     ]
    }
   ],
   "source": [
    "sample_interval = 25\n",
    "checkpoint_interval = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(num_images // batch_size):\n",
    "        x, y = next(data.data_generator())\n",
    "        real_a = Variable(x).to(device)\n",
    "        real_b = Variable(y).to(device)     \n",
    "        valid = Variable(torch.ones((real_a.size(0), 1)), requires_grad=False).to(device)\n",
    "        fake = Variable(torch.zeros((real_a.size(0), 1)), requires_grad=False).to(device)\n",
    "        \n",
    "        # Training Discriminator A with real_A batch\n",
    "        optim_dis_a.zero_grad();\n",
    "        pred_real_dis_a = dis_a(real_a).view(-1, 1)\n",
    "        err_real_dis_a = criterion(pred_real_dis_a, valid)\n",
    "        err_real_dis_a.backward()\n",
    "        \n",
    "        # Training Discriminator B with real_B batch\n",
    "        optim_dis_b.zero_grad();\n",
    "        pred_real_dis_b = dis_b(real_b).view(-1, 1)\n",
    "        err_real_dis_b = criterion(pred_real_dis_b, valid)\n",
    "        err_real_dis_b.backward()\n",
    "        \n",
    "        # Training Discriminator B with fake_B batch of Generator A\n",
    "        fake_b = gen_a(real_a)\n",
    "        pred_fake_dis_b = dis_b(fake_b.detach()).view(-1, 1)\n",
    "        err_fake_dis_b = criterion(pred_fake_dis_b, fake)\n",
    "        err_fake_dis_b.backward()\n",
    "        \n",
    "        # Training Discriminator A with fake_A batch of Generator B\n",
    "        fake_a = gen_b(real_b)\n",
    "        pred_fake_dis_a = dis_a(fake_a.detach()).view(-1, 1)\n",
    "        err_fake_dis_a = criterion(pred_fake_dis_a, fake)\n",
    "        err_fake_dis_a.backward()\n",
    "        \n",
    "        # Update params of Discriminator A and B\n",
    "        err_dis_a = err_real_dis_a + err_fake_dis_a\n",
    "        optim_dis_a.step()\n",
    "        err_dis_b = err_real_dis_b + err_fake_dis_b\n",
    "        optim_dis_b.step()\n",
    "        \n",
    "        # Train and update Generator A based on Discriminator B's prediction\n",
    "        optim_gen_a.zero_grad()\n",
    "        pred_out_dis_b = dis_b(fake_b).view(-1, 1)\n",
    "        err_gen_a = criterion(pred_out_dis_b, valid)\n",
    "        err_gen_a.backward()\n",
    "        optim_gen_a.step()\n",
    "        \n",
    "        # Train and update Generator B based on Discriminator A's prediction\n",
    "        optim_gen_b.zero_grad()\n",
    "        pred_out_dis_a = dis_a(fake_a).view(-1, 1)\n",
    "        err_gen_b = criterion(pred_out_dis_a, valid)\n",
    "        err_gen_b.backward()\n",
    "        optim_gen_b.step()\n",
    "        \n",
    "        # Print statistics and save checkpoints\n",
    "        print(\"\\r[Epoch %d/%d] [Batch %d/%d] [D_A loss: %f] [D_B loss: %f] [G_A loss: %f, G_B loss: %f]\" %\n",
    "                                                        (epoch, num_epochs,\n",
    "                                                        i, num_images//batch_size,\n",
    "                                                        err_dis_a.item(), err_dis_b.item(), \n",
    "                                                        err_gen_a.item(), err_gen_b.item()))\n",
    "\n",
    "        if i % sample_interval == 0:\n",
    "            img_sample = torch.cat((real_a.data, fake_a.data, real_b.data, fake_b.data), -2)\n",
    "            save_image(img_sample, 'saved_images/%s.png' % (epoch + i), nrow=5, normalize=True)\n",
    "\n",
    "\n",
    "    if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
    "        torch.save(gen_a.state_dict(), 'saved_models/generator_a_%d.pth' % (epoch))\n",
    "        torch.save(gen_b.state_dict(), 'saved_models/generator_b_%d.pth' % (epoch))\n",
    "        torch.save(dis_a.state_dict(), 'saved_models/discriminator_a_%d.pth' % (epoch))\n",
    "        torch.save(dis_b.state_dict(), 'saved_models/discriminator_b_%d.pth' % (epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
